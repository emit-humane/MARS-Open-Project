# ğŸ¤ Speech Emotion Recognition  
### MARS Open Project 2025 â€“ Project 1 Submission  
**Author:** Rudra Sharma  
**Contact:** rudra310sharma@gmail.com  

[ğŸ“¹ Watch Demo Video([https://drive.google.com/file/d/1adyPLr-APf5GupYBAbAqJzR5N_2CpJsU/view?usp=drive_link](https://drive.google.com/file/d/1fXiay_vdp7YG4n3to4Ll_GnfRyOp_Oo-/view?usp=drive_link))]

---

## ğŸ§  Project Overview

This project implements a complete pipeline to classify **human emotions from speech** using deep learning. Built using the **RAVDESS dataset**, it leverages **feature extraction with Librosa** and a custom-trained **1D Convolutional Neural Network (Conv1D)** model.

> Predict emotions like: `neutral`, `calm`, `happy`, `sad`, `angry`, `fear`, `disgust`, `surprise`

---

## ğŸ¯ Objective

Build a robust, modular, and interactive pipeline that:
- Extracts acoustic features from `.wav` audio files
- Augments and preprocesses data
- Trains a deep learning model for emotion classification
- Supports both **web-based (Streamlit)** and **CLI-based** prediction

---

## ğŸ“ Dataset

**Dataset Used:** [RAVDESS â€“ Ryerson Audio-Visual Database of Emotional Speech and Song](https://zenodo.org/record/1188976)  
- Files: `Audio_Speech_Actors_01-24`, `Audio_Song_Actors_01-24`
- Each file encodes emotion metadata in its filename

---

## ğŸ“Š Feature Extraction

We use `librosa` to extract meaningful audio features:
- **MFCCs** â€“ Mel-Frequency Cepstral Coefficients (40)
- **Chroma** â€“ Pitch class information
- **Mel Spectrogram**
- **RMS Energy**
- **Zero-Crossing Rate**

ğŸ‘‰ Each audio file is converted into a **162-length feature vector**.

---

## ğŸ§± Model Architecture

Our model is a **deep Conv1D-based neural network**:

Conv1D (256) â†’ MaxPooling â†’
Conv1D (256) â†’ MaxPooling â†’
Conv1D (128) â†’ MaxPooling â†’ Dropout â†’
Conv1D (64) â†’ MaxPooling â†’
Flatten â†’ Dense (32) â†’ Dropout â†’
Output Layer (8 softmax classes)

- **Loss:** Categorical Crossentropy  
- **Optimizer:** Adam  
- **Validation Accuracy:** ~85%  
- **Evaluation:** Confusion matrix, accuracy, F1-score

---

## ğŸ”„ Data Augmentation

Each training sample is augmented into three variations:
1. **Original**
2. **With noise**
3. **Time-stretch + Pitch-shift**

This tripling improves generalization and robustness.

---

## ğŸš€ How to Run

### ğŸ”§ 1. Install Dependencies
```bash
pip install -r requirements.txt

ğŸ” 2. Train the Model (optional)
bash
Copy
Edit
# In Jupyter or Colab
Run model_training.ipynb
# Will generate:
# - emotion_classification_model.h5
# - scaler.pkl
ğŸ“‚ 3. Predict from CLI
python test_model.py path_to_audio.wav
ğŸŒ 4. Run Streamlit App
streamlit run app.py
Upload .wav audio file

Get real-time prediction

ğŸ—‚ Project Structure
emotion-classification/
â”œâ”€â”€ model_training.ipynb            # Full training and preprocessing
â”œâ”€â”€ emotion_classification_model.h5 # Trained Conv1D model
â”œâ”€â”€ scaler.pkl                      # Fitted StandardScaler
â”œâ”€â”€ app.py                          # Streamlit-based frontend
â”œâ”€â”€ test_model.py                   # CLI prediction script
â”œâ”€â”€ requirements.txt                # Dependencies
â””â”€â”€ README.md                       # Youâ€™re here
ğŸ¬ Demo Video
Click here to watch the full demo
Covers:

Model pipeline

Web app prediction

CLI usage

ğŸ“ˆ Performance Metrics
Validation Accuracy: ~85%

Macro F1-Score: >80%

Evaluated using:

![classification_report (precision, recall, f1)](Results.png)

![Confusion Matrix](confusion_matrix.png)


ğŸ”® Future Improvements
ğŸ™ï¸ Live mic recording input

ğŸ“¦ Docker-based deployment

ğŸ›ï¸ Probabilistic class output visualization

ğŸµ Multi-language dataset support

ğŸ“ Deliverables Checklist
Deliverable	Status
Dataset description	âœ…
Feature extraction	âœ…
Model architecture	âœ…
Code and training pipeline	âœ…
Trained model + scaler saved	âœ…
CLI-based inference	âœ…
Streamlit web UI	âœ…
Demo video	âœ…
Clean README.md with instructions	âœ…
